var documenterSearchIndex = {"docs":
[{"location":"hclust.html#Hierarchical-Clustering","page":"Hierarchical Clustering","title":"Hierarchical Clustering","text":"","category":"section"},{"location":"hclust.html","page":"Hierarchical Clustering","title":"Hierarchical Clustering","text":"Hierarchical clustering algorithms build a dendrogram of nested clusters by repeatedly merging or splitting clusters.","category":"page"},{"location":"hclust.html","page":"Hierarchical Clustering","title":"Hierarchical Clustering","text":"The hclust function implements several classical algorithms for hierarchical clustering (the algorithm to use is defined by the linkage parameter):","category":"page"},{"location":"hclust.html","page":"Hierarchical Clustering","title":"Hierarchical Clustering","text":"hclust\nHclust","category":"page"},{"location":"hclust.html#Clustering.hclust","page":"Hierarchical Clustering","title":"Clustering.hclust","text":"hclust(d::AbstractMatrix; [linkage], [uplo], [branchorder]) -> Hclust\n\nPerform hierarchical clustering using the distance matrix d and the cluster linkage function.\n\nReturns the dendrogram as a Hclust object.\n\nArguments\n\nd::AbstractMatrix: the pairwise distance matrix. d_ij is the distance  between i-th and j-th points.\nlinkage::Symbol: cluster linkage function to use. linkage defines how the distances between the data points are aggregated into the distances between the clusters. Naturally, it affects what clusters are merged on each iteration. The valid choices are:\n:single (the default): use the minimum distance between any of the cluster members\n:average: use the mean distance between any of the cluster members\n:complete: use the maximum distance between any of the members\n:ward: the distance is the increase of the average squared distance of a point to its cluster centroid after merging the two clusters\n:ward_presquared: same as :ward, but assumes that the distances in d are already squared.\nuplo::Symbol (optional): specifies whether the upper (:U) or the lower (:L) triangle of d should be used to get the distances. If not specified, the method expects d to be symmetric.\nbranchorder::Symbol (optional): algorithm to order leaves and branches. The valid choices are:\n:r (the default): ordering based on the node heights and the original elements order (compatible with R's hclust)\n:barjoseph (or :optimal): branches are ordered to reduce the distance between neighboring leaves from separate branches using the \"fast optimal leaf ordering\" algorithm from Bar-Joseph et. al. Bioinformatics (2001)\n\n\n\n\n\n","category":"function"},{"location":"hclust.html#Clustering.Hclust","page":"Hierarchical Clustering","title":"Clustering.Hclust","text":"Hclust{T<:Real}\n\nThe output of hclust, hierarchical clustering of data points.\n\nProvides the bottom-up definition of the dendrogram as the sequence of merges of the two lower subtrees into a higher level subtree.\n\nThis type mostly follows R's hclust class.\n\nFields\n\nmerges::Matrix{Int}: N2 matrix encoding subtree merges:\neach row specifies the left and right subtrees (referenced by their ids) that are merged\nnegative subtree id denotes the leaf node and corresponds to the data point at position -id\npositive id denotes nontrivial subtree (the row merges[id, :] specifies its left and right subtrees)\nlinkage::Symbol: the name of cluster linkage function used to construct the hierarchy (see hclust)\nheights::Vector{T}: subtree heights, i.e. the distances between the left  and right branches of each subtree calculated using the specified linkage\norder::Vector{Int}: the data point indices ordered so that there are no  intersecting branches on the dendrogram plot. This ordering also puts  the points of the same cluster close together.\n\nSee also: hclust.\n\n\n\n\n\n","category":"type"},{"location":"hclust.html","page":"Hierarchical Clustering","title":"Hierarchical Clustering","text":"Single-linkage clustering using distance matrix:","category":"page"},{"location":"hclust.html","page":"Hierarchical Clustering","title":"Hierarchical Clustering","text":"using Clustering\nD = rand(1000, 1000);\nD += D'; # symmetric distance matrix (optional)\nresult = hclust(D, linkage=:single)","category":"page"},{"location":"hclust.html","page":"Hierarchical Clustering","title":"Hierarchical Clustering","text":"The resulting dendrogram could be converted into disjoint clusters with the help of cutree function.","category":"page"},{"location":"hclust.html","page":"Hierarchical Clustering","title":"Hierarchical Clustering","text":"cutree","category":"page"},{"location":"hclust.html#Clustering.cutree","page":"Hierarchical Clustering","title":"Clustering.cutree","text":"cutree(hclu::Hclust; [k], [h]) -> Vector{Int}\n\nCut the hclu dendrogram to produce clusters at the specified level of granularity.\n\nReturns the cluster assignments vector z (z_i is the index of the cluster for the i-th data point).\n\nArguments\n\nk::Integer (optional) the number of desired clusters.\nh::Real (optional) the height at which the tree is cut.\n\nIf both k and h are specified, it's guaranteed that the number of clusters is not less than k and their height is not above h.\n\nSee also: hclust\n\n\n\n\n\n","category":"function"},{"location":"init.html#clu_algo_init","page":"Initialization","title":"Initialization","text":"","category":"section"},{"location":"init.html","page":"Initialization","title":"Initialization","text":"A clustering algorithm usually requires initialization before it could be started.","category":"page"},{"location":"init.html#Seeding","page":"Initialization","title":"Seeding","text":"","category":"section"},{"location":"init.html","page":"Initialization","title":"Initialization","text":"Seeding is a type of clustering initialization, which provides a few seeds – points from a data set that would serve as the initial cluster centers (one for each cluster).","category":"page"},{"location":"init.html","page":"Initialization","title":"Initialization","text":"Each seeding algorithm implemented by Clustering.jl is a subtype of SeedingAlgorithm:","category":"page"},{"location":"init.html","page":"Initialization","title":"Initialization","text":"SeedingAlgorithm\ninitseeds!\ninitseeds_by_costs!","category":"page"},{"location":"init.html#Clustering.SeedingAlgorithm","page":"Initialization","title":"Clustering.SeedingAlgorithm","text":"SeedingAlgorithm\n\nBase type for all seeding algorithms.\n\nEach seeding algorithm should implement the two functions: initseeds! and initseeds_by_costs!.\n\n\n\n\n\n","category":"type"},{"location":"init.html#Clustering.initseeds!","page":"Initialization","title":"Clustering.initseeds!","text":"initseeds!(iseeds::AbstractVector{Int}, alg::SeedingAlgorithm,\n           X::AbstractMatrix) -> iseeds\n\nInitialize iseeds with the indices of cluster seeds for the X data matrix using the alg seeding algorithm.\n\n\n\n\n\n","category":"function"},{"location":"init.html#Clustering.initseeds_by_costs!","page":"Initialization","title":"Clustering.initseeds_by_costs!","text":"initseeds_by_costs!(iseeds::AbstractVector{Int}, alg::SeedingAlgorithm,\n                    costs::AbstractMatrix) -> iseeds\n\nInitialize iseeds with the indices of cluster seeds for the costs matrix using the alg seeding algorithm.\n\nHere, costs[i, j] is the cost of assigning points i and j to the same cluster. One may, for example, use the squared Euclidean distance between the points as the cost.\n\n\n\n\n\n","category":"function"},{"location":"init.html","page":"Initialization","title":"Initialization","text":"There are several seeding methods described in the literature. Clustering.jl implements three popular ones:","category":"page"},{"location":"init.html","page":"Initialization","title":"Initialization","text":"KmppAlg\nKmCentralityAlg\nRandSeedAlg","category":"page"},{"location":"init.html#Clustering.KmppAlg","page":"Initialization","title":"Clustering.KmppAlg","text":"KmppAlg <: SeedingAlgorithm\n\nKmeans++ seeding (:kmpp).\n\nChooses the seeds sequentially. The probability of a point to be chosen is proportional to the minimum cost of assigning it to the existing seeds.\n\nReferences\n\nD. Arthur and S. Vassilvitskii (2007). k-means++: the advantages of careful seeding. 18th Annual ACM-SIAM symposium on Discrete algorithms, 2007.\n\n\n\n\n\n","category":"type"},{"location":"init.html#Clustering.KmCentralityAlg","page":"Initialization","title":"Clustering.KmCentralityAlg","text":"KmCentralityAlg <: SeedingAlgorithm\n\nK-medoids initialization based on centrality (:kmcen).\n\nChoose the k points with the highest centrality as seeds.\n\nReferences\n\nHae-Sang Park and Chi-Hyuck Jun. A simple and fast algorithm for K-medoids clustering. doi:10.1016/j.eswa.2008.01.039\n\n\n\n\n\n","category":"type"},{"location":"init.html#Clustering.RandSeedAlg","page":"Initialization","title":"Clustering.RandSeedAlg","text":"RandSeedAlg <: SeedingAlgorithm\n\nRandom seeding (:rand).\n\nChooses an arbitrary subset of k data points as cluster seeds.\n\n\n\n\n\n","category":"type"},{"location":"init.html","page":"Initialization","title":"Initialization","text":"In practice, we have found that Kmeans++ is the most effective choice.","category":"page"},{"location":"init.html","page":"Initialization","title":"Initialization","text":"For convenience, the package defines the two wrapper functions that accept the short name of the seeding algorithm and the number of clusters and take care of allocating iseeds and applying the proper SeedingAlgorithm:","category":"page"},{"location":"init.html","page":"Initialization","title":"Initialization","text":"initseeds\ninitseeds_by_costs","category":"page"},{"location":"init.html#Clustering.initseeds","page":"Initialization","title":"Clustering.initseeds","text":"initseeds(alg::Union{SeedingAlgorithm, Symbol},\n          X::AbstractMatrix, k::Integer) -> Vector{Int}\n\nSelect k seeds from a dn data matrix X using the alg algorithm.\n\nalg could be either an instance of SeedingAlgorithm or a symbolic name of the algorithm.\n\nReturns the vector of k seed indices.\n\n\n\n\n\n","category":"function"},{"location":"init.html#Clustering.initseeds_by_costs","page":"Initialization","title":"Clustering.initseeds_by_costs","text":"initseeds_by_costs(alg::Union{SeedingAlgorithm, Symbol},\n                   costs::AbstractMatrix, k::Integer) -> Vector{Int}\n\nSelect k seeds from the nn costs matrix using algorithm alg.\n\nHere, costs[i, j] is the cost of assigning points iandj` to the same cluster. One may, for example, use the squared Euclidean distance between the points as the cost.\n\nReturns the vector of k seed indices.\n\n\n\n\n\n","category":"function"},{"location":"dbscan.html#DBSCAN","page":"DBSCAN","title":"DBSCAN","text":"","category":"section"},{"location":"dbscan.html","page":"DBSCAN","title":"DBSCAN","text":"Density-based Spatial Clustering of Applications with Noise (DBSCAN) is a data clustering algorithm that finds clusters through density-based expansion of seed points. The algorithm was proposed in:","category":"page"},{"location":"dbscan.html","page":"DBSCAN","title":"DBSCAN","text":"Martin Ester, Hans-peter Kriegel, Jörg S, and Xiaowei Xu A density-based algorithm for discovering clusters in large spatial databases with noise. 1996.","category":"page"},{"location":"dbscan.html#Density-Reachability","page":"DBSCAN","title":"Density Reachability","text":"","category":"section"},{"location":"dbscan.html","page":"DBSCAN","title":"DBSCAN","text":"DBSCAN's definition of a cluster is based on the concept of density reachability: a point q is said to be directly density reachable by another point p if the distance between them is below a specified threshold epsilon and p is surrounded by sufficiently many points. Then, q is considered to be density reachable by p if there exists a sequence p_1 p_2 ldots p_n such that p_1 = p and p_i+1 is directly density reachable from p_i.","category":"page"},{"location":"dbscan.html","page":"DBSCAN","title":"DBSCAN","text":"The points within DBSCAN clusters are categorized into core (or seeds) and boundary:","category":"page"},{"location":"dbscan.html","page":"DBSCAN","title":"DBSCAN","text":"All points of the cluster core are mutually density-connected, meaning that for any two distinct points p and q in a core, there exists a point o such that both p and q are density reachable from o.\nIf a point is density-connected to any point of a cluster core, it is also part of the core.\nAll points within the epsilon-neighborhood of any core point, but not belonging to that core (i.e. not density reachable from the core), are considered cluster boundary.","category":"page"},{"location":"dbscan.html#Interface","page":"DBSCAN","title":"Interface","text":"","category":"section"},{"location":"dbscan.html","page":"DBSCAN","title":"DBSCAN","text":"The implementation of DBSCAN algorithm provided by dbscan function supports the two ways of specifying clustering data:","category":"page"},{"location":"dbscan.html","page":"DBSCAN","title":"DBSCAN","text":"The d times n matrix of point coordinates. This is the preferred method as it uses memory- and time-efficient neighboring points queries via NearestNeighbors.jl package.\nThe ntimes n matrix of precalculated pairwise point distances. It requires O(n^2) memory and time to run.","category":"page"},{"location":"dbscan.html","page":"DBSCAN","title":"DBSCAN","text":"dbscan\nDbscanResult\nDbscanCluster","category":"page"},{"location":"dbscan.html#Clustering.dbscan","page":"DBSCAN","title":"Clustering.dbscan","text":"dbscan(points::AbstractMatrix, radius::Real;\n       [metric=Euclidean()],\n       [min_neighbors=1], [min_cluster_size=1],\n       [nntree_kwargs...]) -> DbscanResult\n\nCluster points using the DBSCAN (Density-Based Spatial Clustering of Applications with Noise) algorithm.\n\nArguments\n\npoints: when metric is specified, the d×n matrix, where each column is a d-dimensional coordinate of a point; when metric=nothing, the n×n matrix of pairwise distances between the points\nradius::Real: neighborhood radius; points within this distance are considered neighbors\n\nOptional keyword arguments to control the algorithm:\n\nmetric (defaults to Euclidean()): the points distance metric to use, nothing means points is the n×n precalculated distance matrix\nmin_neighbors::Integer (defaults to 1): the minimal number of neighbors required to assign a point to a cluster \"core\"\nmin_cluster_size::Integer (defaults to 1): the minimal number of points in a cluster; cluster candidates with fewer points are discarded\nnntree_kwargs...: parameters (like leafsize) for the KDTree constructor\n\nExample\n\npoints = randn(3, 10000)\n# DBSCAN clustering, clusters with less than 20 points will be discarded:\nclustering = dbscan(points, 0.05, min_neighbors = 3, min_cluster_size = 20)\n\nReferences:\n\nMartin Ester, Hans-Peter Kriegel, Jörg Sander, and Xiaowei Xu, \"A density-based algorithm for discovering clusters in large spatial databases with noise\", KDD-1996, pp. 226–231.\nErich Schubert, Jörg Sander, Martin Ester, Hans Peter Kriegel, and Xiaowei Xu, \"DBSCAN Revisited, Revisited: Why and How You Should (Still) Use DBSCAN\", ACM Transactions on Database Systems, Vol.42(3)3, pp. 1–21, https://doi.org/10.1145/3068335\n\n\n\n\n\n","category":"function"},{"location":"dbscan.html#Clustering.DbscanResult","page":"DBSCAN","title":"Clustering.DbscanResult","text":"DbscanResult <: ClusteringResult\n\nThe output of dbscan function.\n\nFields\n\nclusters::Vector{DbscanCluster}: clusters, length K\nseeds::Vector{Int}: indices of the first points of each cluster's core, length K\ncounts::Vector{Int}: cluster sizes (number of assigned points), length K\nassignments::Vector{Int}: vector of clusters indices, where each point was assigned to, length N\n\n\n\n\n\n","category":"type"},{"location":"dbscan.html#Clustering.DbscanCluster","page":"DBSCAN","title":"Clustering.DbscanCluster","text":"DbscanCluster\n\nDBSCAN cluster, part of DbscanResult returned by dbscan function.\n\nFields\n\nsize::Int: number of points in a cluster (core + boundary)\ncore_indices::Vector{Int}: indices of points in the cluster core, a.k.a. seeds  (have at least min_neighbors neighbors in the cluster)\nboundary_indices::Vector{Int}: indices of the cluster points outside of core\n\n\n\n\n\n","category":"type"},{"location":"kmeans.html#K-means","page":"K-means","title":"K-means","text":"","category":"section"},{"location":"kmeans.html","page":"K-means","title":"K-means","text":"K-means is a classical method for clustering or vector quantization. It produces a fixed number of clusters, each associated with a center (also known as a prototype), and each data point is assigned to a cluster with the nearest center.","category":"page"},{"location":"kmeans.html","page":"K-means","title":"K-means","text":"From a mathematical standpoint, K-means is a coordinate descent algorithm that solves the following optimization problem:","category":"page"},{"location":"kmeans.html","page":"K-means","title":"K-means","text":"textminimize  sum_i=1^n  mathbfx_i - boldsymbolmu_z_i ^2  textwrt  (boldsymbolmu z)","category":"page"},{"location":"kmeans.html","page":"K-means","title":"K-means","text":"Here, boldsymbolmu_k is the center of the k-th cluster, and z_i is an index of the cluster for i-th point mathbfx_i.","category":"page"},{"location":"kmeans.html","page":"K-means","title":"K-means","text":"kmeans\nKmeansResult","category":"page"},{"location":"kmeans.html#Clustering.kmeans","page":"K-means","title":"Clustering.kmeans","text":"kmeans(X, k, [...]) -> KmeansResult\n\nK-means clustering of the dn data matrix X (each column of X is a d-dimensional data point) into k clusters.\n\nArguments\n\ninit (defaults to :kmpp): how cluster seeds should be initialized, could be one of the following:\na Symbol, the name of a seeding algorithm (see Seeding for a list of supported methods);\nan instance of SeedingAlgorithm;\nan integer vector of length k that provides the indices of points to use as initial seeds.\nweights: n-element vector of point weights (the cluster centers are the weighted means of cluster members)\nmaxiter, tol, display: see common options\n\n\n\n\n\n","category":"function"},{"location":"kmeans.html#Clustering.KmeansResult","page":"K-means","title":"Clustering.KmeansResult","text":"KmeansResult{C,D<:Real,WC<:Real} <: ClusteringResult\n\nThe output of kmeans and kmeans!.\n\nType parameters\n\nC<:AbstractMatrix{<:AbstractFloat}: type of the centers matrix\nD<:Real: type of the assignment cost\nWC<:Real: type of the cluster weight\n\n\n\n\n\n","category":"type"},{"location":"kmeans.html","page":"K-means","title":"K-means","text":"If you already have a set of initial center vectors, kmeans! could be used:","category":"page"},{"location":"kmeans.html","page":"K-means","title":"K-means","text":"kmeans!","category":"page"},{"location":"kmeans.html#Clustering.kmeans!","page":"K-means","title":"Clustering.kmeans!","text":"kmeans!(X, centers; [kwargs...]) -> KmeansResult\n\nUpdate the current cluster centers (dk matrix, where d is the dimension and k the number of centroids) using the dn data matrix X (each column of X is a d-dimensional data point).\n\nSee kmeans for the description of optional kwargs.\n\n\n\n\n\n","category":"function"},{"location":"kmeans.html#Examples","page":"K-means","title":"Examples","text":"","category":"section"},{"location":"kmeans.html","page":"K-means","title":"K-means","text":"using Clustering\n\n# make a random dataset with 1000 random 5-dimensional points\nX = rand(5, 1000)\n\n# cluster X into 20 clusters using K-means\nR = kmeans(X, 20; maxiter=200, display=:iter)\n\n@assert nclusters(R) == 20 # verify the number of clusters\n\na = assignments(R) # get the assignments of points to clusters\nc = counts(R) # get the cluster sizes\nM = R.centers # get the cluster centers","category":"page"},{"location":"kmeans.html","page":"K-means","title":"K-means","text":"Scatter plot of the K-means clustering results:","category":"page"},{"location":"kmeans.html","page":"K-means","title":"K-means","text":"using RDatasets, Clustering, Plots\niris = dataset(\"datasets\", \"iris\"); # load the data\n\nfeatures = collect(Matrix(iris[:, 1:4])'); # features to use for clustering\nresult = kmeans(features, 3); # run K-means for the 3 clusters\n\n# plot with the point color mapped to the assigned cluster index\nscatter(iris.PetalLength, iris.PetalWidth, marker_z=result.assignments,\n        color=:lightrainbow, legend=false)","category":"page"},{"location":"algorithms.html#clu_algo_basics","page":"Basics","title":"Basics","text":"","category":"section"},{"location":"algorithms.html","page":"Basics","title":"Basics","text":"The package implements a variety of clustering algorithms:","category":"page"},{"location":"algorithms.html","page":"Basics","title":"Basics","text":"Pages = [\"kmeans.md\", \"kmedoids.md\", \"hclust.md\", \"mcl.md\",\n         \"affprop.md\", \"dbscan.md\", \"fuzzycmeans.md\"]","category":"page"},{"location":"algorithms.html","page":"Basics","title":"Basics","text":"Most of the clustering functions in the package have a similar interface, making it easy to switch between different clustering algorithms.","category":"page"},{"location":"algorithms.html#Inputs","page":"Basics","title":"Inputs","text":"","category":"section"},{"location":"algorithms.html","page":"Basics","title":"Basics","text":"A clustering algorithm, depending on its nature, may accept an input matrix in either of the following forms:","category":"page"},{"location":"algorithms.html","page":"Basics","title":"Basics","text":"Data matrix X of size d times n, the i-th column of X (X[:, i]) is a data point (data sample) in d-dimensional space.\nDistance matrix D of size n times n, where D_ij is the distance between the i-th and j-th points, or the cost of assigning them to the same cluster.","category":"page"},{"location":"algorithms.html#common_options","page":"Basics","title":"Common Options","text":"","category":"section"},{"location":"algorithms.html","page":"Basics","title":"Basics","text":"Many clustering algorithms are iterative procedures. The functions share the basic options for controlling the iterations:","category":"page"},{"location":"algorithms.html","page":"Basics","title":"Basics","text":"maxiter::Integer: maximum number of iterations.\ntol::Real: minimal allowed change of the objective during convergence. The algorithm is considered to be converged when the change of objective value between consecutive iterations drops below tol.\ndisplay::Symbol: the level of information to be displayed. It may take one of the following values:\n:none: nothing is shown\n:final: only shows a brief summary when the algorithm ends\n:iter: shows the progress at each iteration","category":"page"},{"location":"algorithms.html#Results","page":"Basics","title":"Results","text":"","category":"section"},{"location":"algorithms.html","page":"Basics","title":"Basics","text":"A clustering function would return an object (typically, an instance of some ClusteringResult subtype) that contains both the resulting clustering (e.g. assignments of points to the clusters) and the information about the clustering algorithm (e.g. the number of iterations and whether it converged).","category":"page"},{"location":"algorithms.html","page":"Basics","title":"Basics","text":"ClusteringResult","category":"page"},{"location":"algorithms.html#Clustering.ClusteringResult","page":"Basics","title":"Clustering.ClusteringResult","text":"ClusteringResult\n\nBase type for the output of clustering algorithm.\n\n\n\n\n\n","category":"type"},{"location":"algorithms.html","page":"Basics","title":"Basics","text":"The following generic methods are supported by any subtype of ClusteringResult:","category":"page"},{"location":"algorithms.html","page":"Basics","title":"Basics","text":"nclusters(::ClusteringResult)\ncounts(::ClusteringResult)\nwcounts(::ClusteringResult)\nassignments(::ClusteringResult)","category":"page"},{"location":"algorithms.html#Clustering.nclusters-Tuple{ClusteringResult}","page":"Basics","title":"Clustering.nclusters","text":"nclusters(R::ClusteringResult) -> Int\n\nGet the number of clusters.\n\n\n\n\n\n","category":"method"},{"location":"algorithms.html#StatsBase.counts-Tuple{ClusteringResult}","page":"Basics","title":"StatsBase.counts","text":"counts(R::ClusteringResult) -> Vector{Int}\n\nGet the vector of cluster sizes.\n\ncounts(R)[k] is the number of points assigned to the k-th cluster.\n\n\n\n\n\n","category":"method"},{"location":"algorithms.html#Clustering.wcounts-Tuple{ClusteringResult}","page":"Basics","title":"Clustering.wcounts","text":"wcounts(R::ClusteringResult) -> Vector{Float64}\nwcounts(R::FuzzyCMeansResult) -> Vector{Float64}\n\nGet the weighted cluster sizes as the sum of weights of points assigned to each cluster.\n\nFor non-weighted clusterings assumes the weight of every data point is 1.0, so the result is equivalent to convert(Vector{Float64}, counts(R)).\n\n\n\n\n\n","category":"method"},{"location":"algorithms.html#Clustering.assignments-Tuple{ClusteringResult}","page":"Basics","title":"Clustering.assignments","text":"assignments(R::ClusteringResult) -> Vector{Int}\n\nGet the vector of cluster indices for each point.\n\nassignments(R)[i] is the index of the cluster to which the i-th point is assigned.\n\n\n\n\n\n","category":"method"},{"location":"kmedoids.html#K-medoids","page":"K-medoids","title":"K-medoids","text":"","category":"section"},{"location":"kmedoids.html","page":"K-medoids","title":"K-medoids","text":"K-medoids is a clustering algorithm that works by finding k data points (called medoids) such that the total distance between each data point and the closest medoid is minimal.","category":"page"},{"location":"kmedoids.html","page":"K-medoids","title":"K-medoids","text":"kmedoids\nkmedoids!\nKmedoidsResult","category":"page"},{"location":"kmedoids.html#Clustering.kmedoids","page":"K-medoids","title":"Clustering.kmedoids","text":"kmedoids(dist::AbstractMatrix, k::Integer; ...) -> KmedoidsResult\n\nPerform K-medoids clustering of n points into k clusters, given the dist matrix (nn, dist[i, j] is the distance between the j-th and i-th points).\n\nArguments\n\ninit (defaults to :kmpp): how medoids should be initialized, could be one of the following:\na Symbol indicating the name of a seeding algorithm (see Seeding for a list of supported methods).\nan integer vector of length k that provides the indices of points to use as initial medoids.\nmaxiter, tol, display: see common options\n\nNote\n\nThe function implements a K-means style algorithm instead of PAM (Partitioning Around Medoids). K-means style algorithm converges in fewer iterations, but was shown to produce worse (10-20% higher total costs) results (see e.g. Schubert & Rousseeuw (2019)).\n\n\n\n\n\n","category":"function"},{"location":"kmedoids.html#Clustering.kmedoids!","page":"K-medoids","title":"Clustering.kmedoids!","text":"kmedoids!(dist::AbstractMatrix, medoids::Vector{Int};\n          [kwargs...]) -> KmedoidsResult\n\nUpdate the current cluster medoids using the dist matrix.\n\nThe medoids field of the returned KmedoidsResult points to the same array as medoids argument.\n\nSee kmedoids for the description of optional kwargs.\n\n\n\n\n\n","category":"function"},{"location":"kmedoids.html#Clustering.KmedoidsResult","page":"K-medoids","title":"Clustering.KmedoidsResult","text":"KmedoidsResult{T} <: ClusteringResult\n\nThe output of kmedoids function.\n\nFields\n\nmedoids::Vector{Int}: the indices of k medoids\nassignments::Vector{Int}: the indices of clusters the points are assigned to, so that medoids[assignments[i]] is the index of the medoid for the i-th point\ncosts::Vector{T}: assignment costs, i.e. costs[i] is the cost of assigning i-th point to its medoid\ncounts::Vector{Int}: cluster sizes\ntotalcost::Float64: total assignment cost (the sum of costs)\niterations::Int: the number of executed algorithm iterations\nconverged::Bool: whether the procedure converged\n\n\n\n\n\n","category":"type"},{"location":"kmedoids.html#kmedoid_refs","page":"K-medoids","title":"References","text":"","category":"section"},{"location":"kmedoids.html","page":"K-medoids","title":"K-medoids","text":"Teitz, M.B. and Bart, P. (1968). Heuristic Methods for Estimating the Generalized Vertex Median of a Weighted Graph. Operations Research, 16(5), 955–961. doi:10.1287/opre.16.5.955\nSchubert, E. and Rousseeuw, P.J. (2019). Faster k-medoids clustering: Improving the PAM, CLARA, and CLARANS Algorithms. SISAP, 171-187. doi:10.1007/978-3-030-32047-8_16","category":"page"},{"location":"affprop.html#Affinity-Propagation","page":"Affinity Propagation","title":"Affinity Propagation","text":"","category":"section"},{"location":"affprop.html","page":"Affinity Propagation","title":"Affinity Propagation","text":"Affinity propagation is a clustering algorithm based on message passing between data points. Similar to K-medoids, it looks at the (dis)similarities in the data, picks one exemplar data point for each cluster, and assigns every point in the data set to the cluster with the closest exemplar.","category":"page"},{"location":"affprop.html","page":"Affinity Propagation","title":"Affinity Propagation","text":"affinityprop\nAffinityPropResult","category":"page"},{"location":"affprop.html#Clustering.affinityprop","page":"Affinity Propagation","title":"Clustering.affinityprop","text":"affinityprop(S::AbstractMatrix; [maxiter=200], [tol=1e-6], [damp=0.5],\n             [display=:none]) -> AffinityPropResult\n\nPerform affinity propagation clustering based on a similarity matrix S.\n\nS_ij (i  j) is the similarity (or the negated distance) between the i-th and j-th points, S_ii defines the availability of the i-th point as an exemplar.\n\nArguments\n\ndamp::Real: the dampening coefficient, 0  mathrmdamp  1. Larger values indicate slower (and probably more stable) update. mathrmdamp = 0 disables dampening.\nmaxiter, tol, display: see common options\n\nReferences\n\nBrendan J. Frey and Delbert Dueck. Clustering by Passing Messages Between Data Points. Science, vol 315, pages 972-976, 2007.\n\n\n\n\n\n","category":"function"},{"location":"affprop.html#Clustering.AffinityPropResult","page":"Affinity Propagation","title":"Clustering.AffinityPropResult","text":"AffinityPropResult <: ClusteringResult\n\nThe output of affinity propagation clustering (affinityprop).\n\nFields\n\nexemplars::Vector{Int}: indices of exemplars (cluster centers)\nassignments::Vector{Int}: cluster assignments for each data point\niterations::Int: number of iterations executed\nconverged::Bool: converged or not\n\n\n\n\n\n","category":"type"},{"location":"validate.html#clu_validate","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"","category":"section"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"Clustering.jl package provides a number of methods to compare different clusterings, evaluate clustering quality or validate its correctness.","category":"page"},{"location":"validate.html#Clustering-comparison","page":"Evaluation & Validation","title":"Clustering comparison","text":"","category":"section"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"Methods to compare two clusterings and measure their similarity.","category":"page"},{"location":"validate.html#Cross-tabulation","page":"Evaluation & Validation","title":"Cross tabulation","text":"","category":"section"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"Cross tabulation, or contingency matrix, is a basis for many clustering quality measures. It shows how similar are the two clusterings on a cluster level.","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"Clustering.jl extends StatsBase.counts() with methods that accept ClusteringResult arguments:","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"counts(::ClusteringResult, ::ClusteringResult)","category":"page"},{"location":"validate.html#StatsBase.counts-Tuple{ClusteringResult, ClusteringResult}","page":"Evaluation & Validation","title":"StatsBase.counts","text":"counts(a::ClusteringResult, b::ClusteringResult) -> Matrix{Int}\ncounts(a::ClusteringResult, b::AbstractVector{<:Integer}) -> Matrix{Int}\ncounts(a::AbstractVector{<:Integer}, b::ClusteringResult) -> Matrix{Int}\n\nCalculate the cross tabulation (aka contingency matrix) for the two clusterings of the same data points.\n\nReturns the n_a  n_b matrix C, where n_a and n_b are the numbers of clusters in a and b, respectively, and C[i, j] is the size of the intersection of i-th cluster from a and j-th cluster from b.\n\nThe clusterings could be specified either as ClusteringResult instances or as vectors of data point assignments.\n\nSee also\n\nconfusion(a::ClusteringResult, a::ClusteringResult) for 2×2 confusion matrix.\n\n\n\n\n\n","category":"method"},{"location":"validate.html#Confusion-matrix","page":"Evaluation & Validation","title":"Confusion matrix","text":"","category":"section"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"Confusion matrix for the two clusterings is a 2×2 contingency table that counts how frequently the pair of data points are in the same or different clusters.","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"confusion","category":"page"},{"location":"validate.html#Clustering.confusion","page":"Evaluation & Validation","title":"Clustering.confusion","text":"confusion([T = Int],\n          a::Union{ClusteringResult, AbstractVector},\n          b::Union{ClusteringResult, AbstractVector}) -> Matrix{T}\n\nCalculate the confusion matrix of the two clusterings.\n\nReturns the 2×2 confusion matrix C of type T (Int by default) that represents partition co-occurrence or similarity matrix between two clusterings a and b by considering all pairs of samples and counting pairs that are assigned into the same or into different clusters.\n\nConsidering a pair of samples that is in the same group as a positive pair, and a pair is in the different group as a negative pair, then the count of true positives is C₁₁, false negatives is C₁₂, false positives C₂₁, and true negatives is C₂₂:\n\n Positive Negative\nPositive C₁₁ C₁₂\nNegative C₂₁ C₂₂\n\nSee also\n\ncounts(a::ClusteringResult, a::ClusteringResult) for full contingency matrix.\n\n\n\n\n\n","category":"function"},{"location":"validate.html#Rand-index","page":"Evaluation & Validation","title":"Rand index","text":"","category":"section"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"Rand index is a measure of the similarity between the two data clusterings. From a mathematical standpoint, Rand index is related to the prediction accuracy, but is applicable even when the original class labels are not used.","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"randindex","category":"page"},{"location":"validate.html#Clustering.randindex","page":"Evaluation & Validation","title":"Clustering.randindex","text":"randindex(a, b) -> NTuple{4, Float64}\n\nCompute the tuple of Rand-related indices between the clusterings c1 and c2.\n\na and b can be either ClusteringResult instances or assignments vectors (AbstractVector{<:Integer}).\n\nReturns a tuple of indices:\n\nHubert & Arabie Adjusted Rand index\nRand index (agreement probability)\nMirkin's index (disagreement probability)\nHubert's index (P(mathrmagree) - P(mathrmdisagree))\n\nReferences\n\nLawrence Hubert and Phipps Arabie (1985). Comparing partitions. Journal of Classification 2 (1): 193-218\n\nMeila, Marina (2003). Comparing Clusterings by the Variation of Information. Learning Theory and Kernel Machines: 173-187.\n\nSteinley, Douglas (2004). Properties of the Hubert-Arabie Adjusted Rand Index. Psychological Methods, Vol. 9, No. 3: 386-396\n\n\n\n\n\n","category":"function"},{"location":"validate.html#Variation-of-Information","page":"Evaluation & Validation","title":"Variation of Information","text":"","category":"section"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"Variation of information (also known as shared information distance) is a measure of the distance between the two clusterings. It is devised from the mutual information, but it is a true metric, i.e. it is symmetric and satisfies the triangle inequality.","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"Clustering.varinfo","category":"page"},{"location":"validate.html#Clustering.varinfo","page":"Evaluation & Validation","title":"Clustering.varinfo","text":"varinfo(a, b) -> Float64\n\nCompute the variation of information between the two clusterings of the same data points.\n\na and b can be either ClusteringResult instances or assignments vectors (AbstractVector{<:Integer}).\n\nReferences\n\nMeila, Marina (2003). Comparing Clusterings by the Variation of Information. Learning Theory and Kernel Machines: 173–187.\n\n\n\n\n\n","category":"function"},{"location":"validate.html#V-measure","page":"Evaluation & Validation","title":"V-measure","text":"","category":"section"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"V-measure can be used to compare the clustering results with the existing class labels of data points or with the alternative clustering. It is defined as the harmonic mean of homogeneity (h) and completeness (c) of the clustering:","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"V_beta = (1+beta)frach cdot cbeta cdot h + c","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"Both h and c can be expressed in terms of the mutual information and entropy measures from the information theory. Homogeneity (h) is maximized when each cluster contains elements of as few different classes as possible. Completeness (c) aims to put all elements of each class in single clusters. The beta parameter (beta  0) could used to control the weights of h and c in the final measure. If beta  1, completeness has more weight, and when beta  1 it's homogeneity.","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"vmeasure","category":"page"},{"location":"validate.html#Clustering.vmeasure","page":"Evaluation & Validation","title":"Clustering.vmeasure","text":"vmeasure(a, b; [β = 1.0]) -> Float64\n\nV-measure between the two clusterings.\n\na and b can be either ClusteringResult instances or assignments vectors (AbstractVector{<:Integer}).\n\nThe β parameter defines trade-off between homogeneity and completeness:\n\nif β  1, completeness is weighted more strongly,\nif β  1, homogeneity is weighted more strongly.\n\nReferences\n\nAndrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A conditional entropy-based external cluster evaluation measure\n\n\n\n\n\n","category":"function"},{"location":"validate.html#Mutual-information","page":"Evaluation & Validation","title":"Mutual information","text":"","category":"section"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"Mutual information quantifies the \"amount of information\" obtained about one random variable through observing the other random variable. It is used in determining the similarity of two different clusterings of a dataset.","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"mutualinfo","category":"page"},{"location":"validate.html#Clustering.mutualinfo","page":"Evaluation & Validation","title":"Clustering.mutualinfo","text":"mutualinfo(a, b; normed=true) -> Float64\n\nCompute the mutual information between the two clusterings of the same data points.\n\na and b can be either ClusteringResult instances or assignments vectors (AbstractVector{<:Integer}).\n\nIf normed parameter is true the return value is the normalized mutual information (symmetric uncertainty), see \"Data Mining Practical Machine Tools and Techniques\", Witten & Frank 2005.\n\nReferences\n\nVinh, Epps, and Bailey, (2009). Information theoretic measures for clusterings comparison. Proceedings of the 26th Annual International Conference on Machine Learning - ICML ‘09.\n\n\n\n\n\n","category":"function"},{"location":"validate.html#Clustering-quality-indices","page":"Evaluation & Validation","title":"Clustering quality indices","text":"","category":"section"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"clustering_quality() methods allow computing intrinsic clustering quality indices, i.e. the metrics that depend only on the clustering itself and do not use the external knowledge. These metrics can be used to compare different clustering algorithms or choose the optimal number of clusters.","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"quality index quality_index option clustering type better quality cluster centers\nCalinski-Harabasz :calinsky_harabasz hard/fuzzy higher values required\nXie-Beni :xie_beni hard/fuzzy lower values required\nDavis-Bouldin :davis_bouldin hard lower values required\nDunn :dunn hard higher values not required\nsilhouettes :silhouettes hard higher values not required","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"clustering_quality","category":"page"},{"location":"validate.html#Clustering.clustering_quality","page":"Evaluation & Validation","title":"Clustering.clustering_quality","text":"For \"hard\" clustering:\n\nclustering_quality(data, centers, assignments; quality_index, [metric])\nclustering_quality(data, clustering; quality_index, [metric])\n\nFor fuzzy (\"soft\") clustering:\n\nclustering_quality(data, centers, weights; quality_index, fuzziness, [metric])\nclustering_quality(data, clustering; quality_index, fuzziness, [metric])\n\nFor \"hard\" clustering without specifying cluster centers:\n\nclustering_quality(data, assignments; quality_index, [metric])\nclustering_quality(data, clustering; quality_index, [metric])\n\nFor \"hard\" clustering without specifying data points and cluster centers:\n\nclustering_quality(assignments, dist_matrix; quality_index)\nclustering_quality(clustering, dist_matrix; quality_index)\n\nCompute the quality index for a given clustering.\n\nReturns a quality index (real value).\n\nArguments\n\ndata::AbstractMatrix: dn data matrix with each column representing one d-dimensional data point\ncenters::AbstractMatrix: dk matrix with cluster centers represented as columns\nassignments::AbstractVector{Int}: n vector of point assignments (cluster indices)\nweights::AbstractMatrix: nk matrix with fuzzy clustering weights, weights[i,j] is the degree of membership of i-th data point to j-th cluster\nclustering::Union{ClusteringResult, FuzzyCMeansResult}: the output of the clustering method\nquality_index::Symbol: quality index to calculate; see below for the supported options\ndist_matrix::AbstractMatrix: a nn pairwise distance matrix; dist_matrix[i,j] is the distance between i-th and j-th points\n\nKeyword arguments\n\nquality_index::Symbol: clustering quality index to calculate; see below for the supported options\nfuzziness::Real: clustering fuzziness > 1\nmetric::SemiMetric=SqEuclidean(): SemiMetric object that defines the metric/distance/similarity function\n\nWhen calling clustering_quality, one can explicitly specify centers, assignments, and weights, or provide ClusteringResult via clustering, from which the necessary data will be read automatically.\n\nFor clustering without known cluster centers the data points are not required. dist_matrix could be provided explicitly, otherwise it would be calculated from the data points using the specified metric.\n\nSupported quality indices\n\n:calinski_harabasz: hard or fuzzy Calinski-Harabsz index (↑), the corrected ratio of between cluster centers inertia and within-clusters inertia\n:xie_beni: hard or fuzzy Xie-Beni index (↓), the ratio betwen inertia within clusters and minimal distance between the cluster centers\n:davies_bouldin: Davies-Bouldin index (↓), the similarity between the cluster and the other most similar one, averaged over all clusters\n:dunn: Dunn index (↑), the ratio of the minimal distance between clusters and the maximal cluster diameter\n:silhouettes: the average silhouette index (↑), see silhouettes\n\nThe arrows ↑ or ↓ specify the direction of the incresing clustering quality. Please refer to the documentation for more details on the clustering quality indices.\n\n\n\n\n\n","category":"function"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"The clustering quality index definitions use the following notation:","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"x_1 x_2 ldots x_n: data points,\nC_1 C_2 ldots C_k: clusters,\nc_j and c: cluster centers and global dataset center,\nd: a similarity (distance) function,\nw_ij: weights measuring membership of a point x_i to a cluster C_j,\nalpha:  a fuzziness parameter.","category":"page"},{"location":"validate.html#calinsky_harabasz","page":"Evaluation & Validation","title":"Calinski-Harabasz index","text":"","category":"section"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"Calinski-Harabasz index (option :calinski_harabasz) measures corrected ratio between global inertia of the cluster centers and the summed internal inertias of clusters:","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"fracn-kk-1fracsum_C_jC_jd(c_jc)sumlimits_C_jsumlimits_x_iin C_j d(x_ic_j) quad textandquad\nfracn-kk-1 fracsumlimits_C_jleft(sumlimits_x_iw_ij^alpharight) d(c_jc)sum_C_j sum_x_i w_ij^alpha d(x_ic_j)","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"for hard and fuzzy (soft) clusterings, respectively. Higher values indicate better quality.","category":"page"},{"location":"validate.html#xie_beni","page":"Evaluation & Validation","title":"Xie-Beni index","text":"","category":"section"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"Xie-Beni index (option :xie_beni) measures ratio between summed inertia of clusters and the minimum distance between cluster centres:","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"fracsum_C_jsum_x_iin C_jd(x_ic_j)nminlimits_c_j_1neq c_j_2 d(c_j_1c_j_2) \nquad textandquad\nfracsum_C_jsum_x_i w_ij^alpha d(x_ic_j)nminlimits_c_j_1neq c_j_2 d(c_j_1c_j_2) ","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"for hard and fuzzy (soft) clusterings, respectively. Lower values indicate better quality.","category":"page"},{"location":"validate.html#davis_bouldin","page":"Evaluation & Validation","title":"Davis-Bouldin index","text":"","category":"section"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"Davis-Bouldin index (option :davis_bouldin) measures average cohesion based on the cluster diameters and distances between cluster centers:","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"frac1ksum_C_j_1max_c_j_2neq c_j_1fracS(C_j_1)+S(C_j_2)d(c_j_1c_j_2)","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"where","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"S(C_j) = frac1C_jsum_x_iin C_jd(x_ic_j)","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"Lower values indicate better quality.","category":"page"},{"location":"validate.html#dunn","page":"Evaluation & Validation","title":"Dunn index","text":"","category":"section"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"Dunn index (option :dunn) measures the ratio between the nearest neighbour distance divided by the maximum cluster diameter:","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"fracminlimits_ C_j_1neq C_j_2 mathrmdist(C_j_1C_j_2)maxlimits_C_jmathrmdiam(C_j)","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"where","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"mathrmdist(C_j_1C_j_2) = minlimits_x_i_1in C_j_1x_i_2in C_j_2 d(x_i_1x_i_2)quad mathrmdiam(C_j) = maxlimits_x_i_1x_i_2in C_j d(x_i_1x_i_2)","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"It is more computationally demanding quality index, which can be used when the centres are not known. Higher values indicate better quality.","category":"page"},{"location":"validate.html#silhouettes_index","page":"Evaluation & Validation","title":"Silhouettes","text":"","category":"section"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"Silhouettes metric quantifies the correctness of point-to-cluster asssignment by comparing the distance of the point to its cluster and to the other clusters.","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"The Silhouette value for the i-th data point is:","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"s_i = fracb_i - a_imax(a_i b_i)  textwhere","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"a_i is the average distance from the i-th point to the other points in the same cluster z_i,\nb_i  min_k ne z_i b_ik, where b_ik is the average distance from the i-th point to the points in the k-th cluster.","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"Note that s_i le 1, and that s_i is close to 1 when the i-th point lies well within its own cluster. This property allows using average silhouette value mean(silhouettes(assignments, counts, X)) as a measure of clustering quality; it is also available using clustering_quality(...; quality_index = :silhouettes) method. Higher values indicate better separation of clusters w.r.t. point distances.","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"silhouettes","category":"page"},{"location":"validate.html#Clustering.silhouettes","page":"Evaluation & Validation","title":"Clustering.silhouettes","text":"silhouettes(assignments::Union{AbstractVector, ClusteringResult}, point_dists::Matrix) -> Vector{Float64}\nsilhouettes(assignments::Union{AbstractVector, ClusteringResult}, points::Matrix;\n            metric::SemiMetric, [batch_size::Integer]) -> Vector{Float64}\n\nCompute silhouette values for individual points w.r.t. given clustering.\n\nReturns the n-length vector of silhouette values for each individual point.\n\nArguments\n\nassignments::Union{AbstractVector{Int}, ClusteringResult}: the vector of point assignments (cluster indices)\npoints::AbstractMatrix: if metric is nothing it is an nn matrix of pairwise distances between the points, otherwise it is an dn matrix of d dimensional clustered data points.\nmetric::Union{SemiMetric, Nothing}: an instance of Distances Metric object or nothing, indicating the distance metric used for calculating point distances.\nbatch_size::Union{Integer, Nothing}: if integer is given, calculate silhouettes in batches of batch_size points each, throws DimensionMismatch if batched calculation is not supported by given metric.\n\nReferences\n\nPeter J. Rousseeuw (1987). Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster Analysis. Computational and Applied Mathematics. 20: 53–65. Marco Gaido (2023). Distributed Silhouette Algorithm: Evaluating Clustering on Big Data\n\n\n\n\n\n","category":"function"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"clustering_quality(..., quality_index=:silhouettes) provides mean silhouette metric for the datapoints. Higher values indicate better quality.","category":"page"},{"location":"validate.html#References","page":"Evaluation & Validation","title":"References","text":"","category":"section"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"Olatz Arbelaitz et al. (2013). An extensive comparative study of cluster validity indices. Pattern Recognition. 46 1: 243-256. doi:10.1016/j.patcog.2012.07.021","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"Aybükë Oztürk, Stéphane Lallich, Jérôme Darmont. (2018). A Visual Quality Index for Fuzzy C-Means.  14th International Conference on Artificial Intelligence Applications and Innovations (AIAI 2018). 546-555. doi:10.1007/978-3-319-92007-8_46.","category":"page"},{"location":"validate.html#Examples","page":"Evaluation & Validation","title":"Examples","text":"","category":"section"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"Exemplary data with 3 real clusters.","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"using Plots, Plots.PlotMeasures, Clustering\nX_clusters = [(center = [4., 5.], std = 0.4, n = 10),\n              (center = [9., -5.], std = 0.4, n = 5),\n              (center = [-4., -9.], std = 1, n = 5)]\nX = mapreduce(hcat, X_clusters) do (center, std, n)\n    center .+ std .* randn(length(center), n)\nend\nX_assignments = mapreduce(vcat, enumerate(X_clusters)) do (i, (_, _, n))\n    fill(i, n)\nend\n\nscatter(view(X, 1, :), view(X, 2, :),\n    markercolor = X_assignments,\n    plot_title = \"Data\", label = nothing,\n    xlabel = \"x\", ylabel = \"y\",\n    legend = :outerright,\n    size = (600, 500)\n);\nsavefig(\"clu_quality_data.svg\"); nothing # hide","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"(Image: )","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"Hard clustering quality for K-means method with 2 to 5 clusters:","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"hard_nclusters = 2:5\nclusterings = kmeans.(Ref(X), hard_nclusters)\n\nplot((\n    plot(hard_nclusters,\n         clustering_quality.(Ref(X), clusterings, quality_index = qidx),\n         marker = :circle,\n         title = \":$qidx\", label = nothing,\n    ) for qidx in [:silhouettes, :calinski_harabasz, :xie_beni, :davies_bouldin, :dunn])...,\n    layout = (2, 3),\n    xaxis = \"N clusters\", yaxis = \"Quality\",\n    plot_title = \"\\\"Hard\\\" clustering quality indices\",\n    size = (1000, 600), left_margin = 10pt\n)\nsavefig(\"clu_quality_hard.svg\"); nothing # hide","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"(Image: )","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"Fuzzy clustering quality for fuzzy C-means method with 2 to 5 clusters:","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"fuzziness = [1.3 2 3]\nfuzzy_nclusters = 2:5\nfuzzy_clusterings = fuzzy_cmeans.(Ref(X), fuzzy_nclusters, fuzziness)\n\nplot((\n    plot(fuzzy_nclusters,\n         [clustering_quality.(Ref(X), fuzz_clusterings,\n                              fuzziness = fuzz, quality_index = qidx)\n          for (fuzz, fuzz_clusterings) in zip(fuzziness, eachcol(fuzzy_clusterings))],\n         marker = :circle,\n         title = \":$qidx\", label = [\"Fuzziness $fuzz\" for fuzz in fuzziness],\n    ) for qidx in [:calinski_harabasz, :xie_beni])...,\n    layout = (1, 2), legend = :left,\n    xaxis = \"N clusters\", yaxis = \"Quality\",\n    plot_title = \"\\\"Soft\\\" clustering quality indices\",\n    size = (700, 350), left_margin = 10pt\n)\nsavefig(\"clu_quality_soft.svg\"); nothing # hide","category":"page"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"(Image: )","category":"page"},{"location":"validate.html#Other-packages","page":"Evaluation & Validation","title":"Other packages","text":"","category":"section"},{"location":"validate.html","page":"Evaluation & Validation","title":"Evaluation & Validation","text":"ClusteringBenchmarks.jl provides benchmark datasets and implements additional methods for evaluating clustering performance.","category":"page"},{"location":"mcl.html#MCL-(Markov-Cluster-Algorithm)","page":"MCL (Markov Cluster Algorithm)","title":"MCL (Markov Cluster Algorithm)","text":"","category":"section"},{"location":"mcl.html","page":"MCL (Markov Cluster Algorithm)","title":"MCL (Markov Cluster Algorithm)","text":"Markov Cluster Algorithm works by simulating a stochastic (Markov) flow in a weighted graph, where each node is a data point, and the edge weights are defined by the adjacency matrix. ... When the algorithm converges, it produces the new edge weights that define the new connected components of the graph (i.e. the clusters).","category":"page"},{"location":"mcl.html","page":"MCL (Markov Cluster Algorithm)","title":"MCL (Markov Cluster Algorithm)","text":"mcl\nMCLResult","category":"page"},{"location":"mcl.html#Clustering.mcl","page":"MCL (Markov Cluster Algorithm)","title":"Clustering.mcl","text":"mcl(adj::AbstractMatrix; [kwargs...]) -> MCLResult\n\nPerform MCL (Markov Cluster Algorithm) clustering using nn adjacency (points similarity) matrix adj.\n\nArguments\n\nKeyword arguments to control the MCL algorithm:\n\nadd_loops::Bool (enabled by default): whether the edges of weight 1.0 from the node to itself should be appended to the graph\nexpansion::Number (defaults to 2): MCL expansion constant\ninflation::Number (defaults to 2): MCL inflation constant\nsave_final_matrix::Bool (disabled by default): whether to save the final equilibrium state in the mcl_adj field of the result; could provide useful diagnostic if the method doesn't converge\nprune_tol::Number: pruning threshold\ndisplay, maxiter, tol: see common options\n\nReferences\n\nStijn van Dongen, \"Graph clustering by flow simulation\", 2001\n\nOriginal MCL implementation.\n\n\n\n\n\n","category":"function"},{"location":"mcl.html#Clustering.MCLResult","page":"MCL (Markov Cluster Algorithm)","title":"Clustering.MCLResult","text":"MCLResult <: ClusteringResult\n\nThe output of mcl function.\n\nFields\n\nmcl_adj::AbstractMatrix: the final MCL adjacency matrix (equilibrium state matrix if the algorithm converged), empty if save_final_matrix option is disabled\nassignments::Vector{Int}: indices of the points clusters. assignments[i] is the index of the cluster for the i-th point  (0 if unassigned)\ncounts::Vector{Int}: the k-length vector of cluster sizes\nnunassigned::Int: the number of standalone points not assigned to any cluster\niterations::Int: the number of elapsed iterations\nrel_Δ::Float64: the final relative Δ\nconverged::Bool: whether the method converged\n\n\n\n\n\n","category":"type"},{"location":"index.html#Clustering.jl-package","page":"Introduction","title":"Clustering.jl package","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Clustering.jl is a Julia package for data clustering. It covers the two aspects of data clustering:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Clustering Algorithms: K-means, K-medoids, Affinity propagation, DBSCAN etc.\nClustering Comparison & Evaluation: cross-tabulation, variational and mutual information, intrinsic clustering quality indices, such as silhouettes, etc.","category":"page"},{"location":"fuzzycmeans.html#fuzzy_cmeans_def","page":"Fuzzy C-means","title":"Fuzzy C-means","text":"","category":"section"},{"location":"fuzzycmeans.html","page":"Fuzzy C-means","title":"Fuzzy C-means","text":"Fuzzy C-means is a clustering method that provides cluster membership weights instead of \"hard\" classification (e.g. K-means).","category":"page"},{"location":"fuzzycmeans.html","page":"Fuzzy C-means","title":"Fuzzy C-means","text":"From a mathematical standpoint, fuzzy C-means solves the following optimization problem:","category":"page"},{"location":"fuzzycmeans.html","page":"Fuzzy C-means","title":"Fuzzy C-means","text":"argmin_mathcalC  sum_i=1^n sum_j=1^C w_ij^mu  mathbfx_i - mathbfc_j ^2 \ntextwhere w_ij = left(sum_k=1^C left(fracleftmathbfx_i - mathbfc_j rightleftmathbfx_i - mathbfc_k rightright)^frac2mu-1right)^-1","category":"page"},{"location":"fuzzycmeans.html","page":"Fuzzy C-means","title":"Fuzzy C-means","text":"Here, mathbfc_j is the center of the j-th cluster, w_ij is the membership weight of the i-th point in the j-th cluster, and mu  1 is a user-defined fuzziness parameter.","category":"page"},{"location":"fuzzycmeans.html","page":"Fuzzy C-means","title":"Fuzzy C-means","text":"fuzzy_cmeans\nFuzzyCMeansResult\nwcounts","category":"page"},{"location":"fuzzycmeans.html#Clustering.fuzzy_cmeans","page":"Fuzzy C-means","title":"Clustering.fuzzy_cmeans","text":"fuzzy_cmeans(data::AbstractMatrix, C::Integer, fuzziness::Real;\n             [dist_metric::SemiMetric], [...]) -> FuzzyCMeansResult\n\nPerform Fuzzy C-means clustering over the given data.\n\nArguments\n\ndata::AbstractMatrix: dn data matrix. Each column represents one d-dimensional data point.\nC::Integer: the number of fuzzy clusters, 2  C  n.\nfuzziness::Real: clusters fuzziness (μ in the mathematical formulation), μ  1.\n\nOptional keyword arguments:\n\ndist_metric::SemiMetric (defaults to Euclidean): the SemiMetric object  that defines the distance between the data points\nmaxiter, tol, display, rng: see common options\n\n\n\n\n\n","category":"function"},{"location":"fuzzycmeans.html#Clustering.FuzzyCMeansResult","page":"Fuzzy C-means","title":"Clustering.FuzzyCMeansResult","text":"FuzzyCMeansResult{T<:AbstractFloat}\n\nThe output of fuzzy_cmeans function.\n\nFields\n\ncenters::Matrix{T}: the dC matrix with columns being the centers of resulting fuzzy clusters\nweights::Matrix{Float64}: the nC matrix of assignment weights (mathrmweights_ij is the weight (probability) of assigning i-th point to the j-th cluster)\niterations::Int: the number of executed algorithm iterations\nconverged::Bool: whether the procedure converged\n\n\n\n\n\n","category":"type"},{"location":"fuzzycmeans.html#Clustering.wcounts","page":"Fuzzy C-means","title":"Clustering.wcounts","text":"wcounts(R::ClusteringResult) -> Vector{Float64}\nwcounts(R::FuzzyCMeansResult) -> Vector{Float64}\n\nGet the weighted cluster sizes as the sum of weights of points assigned to each cluster.\n\nFor non-weighted clusterings assumes the weight of every data point is 1.0, so the result is equivalent to convert(Vector{Float64}, counts(R)).\n\n\n\n\n\n","category":"function"},{"location":"fuzzycmeans.html#Examples","page":"Fuzzy C-means","title":"Examples","text":"","category":"section"},{"location":"fuzzycmeans.html","page":"Fuzzy C-means","title":"Fuzzy C-means","text":"using Clustering\n\n# make a random dataset with 1000 points\n# each point is a 5-dimensional vector\nX = rand(5, 1000)\n\n# performs Fuzzy C-means over X, trying to group them into 3 clusters\n# with a fuzziness factor of 2. Set maximum number of iterations to 200\n# set display to :iter, so it shows progressive info at each iteration\nR = fuzzy_cmeans(X, 3, 2, maxiter=200, display=:iter)\n\n# get the centers (i.e. weighted mean vectors)\n# M is a 5x3 matrix\n# M[:, k] is the center of the k-th cluster\nM = R.centers\n\n# get the point memberships over all the clusters\n# memberships is a 20x3 matrix\nmemberships = R.weights","category":"page"}]
}
